import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL
from transformers import CLIPTextModel, CLIPTokenizer
import PIL.Image
import numpy as np
import os
import json
from glob import glob
import random
import gc
from torchvision import transforms
from torchvision.transforms.functional import adjust_brightness, adjust_contrast, adjust_saturation


# 1. ç¯å¢ƒç¼–ç å™¨ - å¿…é¡»å…ˆå®šä¹‰
class EnvironmentEncoder(nn.Module):
    def __init__(self, output_dim=2304):
        super().__init__()
        # å‡å°ç½‘ç»œè§„æ¨¡
        self.mlp = nn.Sequential(
            nn.Linear(32 * 32 * 3, 2048),  # å‡å°éšè—å±‚å¤§å°
            nn.LeakyReLU(0.2),
            nn.Linear(2048, 2048),
            nn.LeakyReLU(0.2),
            nn.Linear(2048, output_dim)
        )

    def forward(self, environment_map):
        batch_size = environment_map.shape[0]
        x = environment_map.reshape(batch_size, -1)
        x = self.mlp(x)
        x = x.reshape(batch_size, 3, 768)
        return x

# 2. å¢å¼ºçš„æ•°æ®é›†ç±»
class ICLightDataset(Dataset):
    def __init__(self, data_dir="./training_images", image_size=512, max_samples=200):
        self.data_dir = data_dir
        self.image_size = image_size
        self.max_samples = max_samples
        self.image_paths = self.find_images()[:max_samples]

        # æ•°æ®å¢å¼ºå˜æ¢
        self.transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
        ])

        # å…‰ç…§ç›¸å…³çš„æç¤ºè¯
        self.lighting_prompts = [
            "professional studio lighting", "natural sunlight illumination",
            "dramatic cinematic lighting", "soft ambient light",
            "warm golden hour lighting", "cool blue hour lighting",
            "harsh direct lighting", "soft diffused lighting",
            "backlit silhouette", "side lighting with shadows",
            "top down lighting", "rim lighting effect",
            "moody low key lighting", "bright high key lighting",
            "sunset glow lighting", "morning mist lighting"
        ]

    def find_images(self):
        """æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶"""
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        image_paths = []
        for ext in extensions:
            image_paths.extend(glob(os.path.join(self.data_dir, ext)))
            image_paths.extend(glob(os.path.join(self.data_dir, ext.upper())))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        """è·å–å•ä¸ªè®­ç»ƒæ ·æœ¬"""
        # 1. è·å–å›¾åƒè·¯å¾„
        img_path = self.image_paths[idx]

        try:
            # 2. åŠ è½½å›¾åƒ
            image = PIL.Image.open(img_path).convert('RGB')

            # 3. éšæœºè£å‰ªå’Œè°ƒæ•´å¤§å°
            image = self.random_crop_and_resize(image)

            # 4. åº”ç”¨æ•°æ®å¢å¼º
            image = self.transform(image)

            # 5. è½¬æ¢ä¸ºå¼ é‡å¹¶å½’ä¸€åŒ–åˆ° [-1, 1]
            image_tensor = torch.from_numpy(np.array(image)).float() / 255.0
            image_tensor = image_tensor.permute(2, 0, 1) * 2.0 - 1.0

            # 6. åˆ›å»ºå¤šæ ·åŒ–çš„è®­ç»ƒå¯¹
            appearance, degradation, background, mask = self.create_training_pair(image_tensor)

            # 7. ç¡®ä¿æ‰€æœ‰å¼ é‡å½¢çŠ¶æ­£ç¡®

            # 7.1 ç¡®ä¿å›¾åƒæ˜¯3ç»´çš„ [3, H, W]
            if appearance.dim() == 4:  # å¦‚æœæ˜¯ [1, 3, H, W]
                appearance = appearance.squeeze(0)
            if degradation.dim() == 4:
                degradation = degradation.squeeze(0)
            if background.dim() == 4:
                background = background.squeeze(0)

            # 7.2 ç¡®ä¿maskæ˜¯3ç»´çš„ [1, H, W]
            if mask.dim() == 2:  # å¦‚æœæ˜¯ [H, W]
                mask = mask.unsqueeze(0)  # å˜æˆ [1, H, W]
            elif mask.dim() == 3 and mask.shape[0] != 1:
                # å¦‚æœmaskæ˜¯ [C, H, W] ä¸” C != 1
                if mask.shape[0] == 3:  # å¦‚æœæ˜¯RGB mask
                    mask = mask.mean(dim=0, keepdim=True)  # è½¬ä¸ºç°åº¦
                else:
                    mask = mask[:1]  # åªå–ç¬¬ä¸€ä¸ªé€šé“

            # 7.3 ç¡®ä¿maskå€¼åœ¨[0, 1]èŒƒå›´å†…
            mask = torch.clamp(mask, 0, 1)

            # 8. åˆ›å»ºç¯å¢ƒå…‰ç…§å›¾
            environment_map = self.create_environment_map(image_tensor)

            # 8.1 ç¡®ä¿environment_mapæ˜¯3ç»´çš„ [32, 32, 3]
            if environment_map.dim() == 2:  # å¦‚æœæ˜¯ [32, 32]
                environment_map = environment_map.unsqueeze(-1).repeat(1, 1, 3)  # å˜æˆ [32, 32, 3]
            elif environment_map.dim() == 4:  # å¦‚æœæ˜¯ [1, 32, 32, 3]
                environment_map = environment_map.squeeze(0)  # å˜æˆ [32, 32, 3]

            # 8.2 ç¡®ä¿ç¯å¢ƒå›¾å€¼åœ¨[0, 1]èŒƒå›´å†…
            environment_map = torch.clamp(environment_map, 0, 1)

            # 9. éšæœºé€‰æ‹©å…‰ç…§æç¤ºè¯
            prompt = random.choice(self.lighting_prompts)

            # 10. éªŒè¯è¾“å‡ºå½¢çŠ¶ï¼ˆè°ƒè¯•ç”¨ï¼Œåªåœ¨å‰å‡ ä¸ªæ ·æœ¬æ˜¾ç¤ºï¼‰
            if idx < 3:
                print(f"\nğŸ“Š æ•°æ®é›†æ ·æœ¬ {idx} å½¢çŠ¶æ£€æŸ¥:")
                print(f"  ğŸ¯ appearance:      {appearance.shape}      (åº”ä¸º [3, {self.image_size}, {self.image_size}])")
                print(f"  ğŸ”§ degradation:     {degradation.shape}     (åº”ä¸º [3, {self.image_size}, {self.image_size}])")
                print(
                    f"  ğŸï¸  background:      {background.shape}      (åº”ä¸º [3, {self.image_size}, {self.image_size}])")
                print(f"  ğŸ­ mask:            {mask.shape}            (åº”ä¸º [1, {self.image_size}, {self.image_size}])")
                print(f"  ğŸ’¡ environment_map: {environment_map.shape} (åº”ä¸º [32, 32, 3])")
                print(f"  ğŸ“ prompt:          '{prompt}'")

                # é¢å¤–æ£€æŸ¥å€¼èŒƒå›´
                print(f"  ğŸ“ˆ å€¼èŒƒå›´æ£€æŸ¥:")
                print(f"     appearanceèŒƒå›´: [{appearance.min():.2f}, {appearance.max():.2f}] (åº”ä¸º [-1, 1])")
                print(f"     maskèŒƒå›´: [{mask.min():.2f}, {mask.max():.2f}] (åº”ä¸º [0, 1])")
                print(
                    f"     environment_mapèŒƒå›´: [{environment_map.min():.2f}, {environment_map.max():.2f}] (åº”ä¸º [0, 1])")

            # 11. è¿”å›æ ·æœ¬å­—å…¸
            return {
                'appearance': appearance,  # å½¢çŠ¶: [3, H, W]
                'degradation': degradation,  # å½¢çŠ¶: [3, H, W]
                'background': background,  # å½¢çŠ¶: [3, H, W]
                'mask': mask,  # å½¢çŠ¶: [1, H, W]
                'environment_map': environment_map,  # å½¢çŠ¶: [32, 32, 3]
                'prompt': prompt  # ç±»å‹: str
            }

        except Exception as e:
            # 12. é”™è¯¯å¤„ç†
            print(f"\nâŒ å¤„ç†å›¾åƒ {img_path} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

            # 13. è¿”å›é»˜è®¤æ ·æœ¬
            print("ğŸ”„ è¿”å›é»˜è®¤æ ·æœ¬...")
            return self.create_default_sample()

    def random_crop_and_resize(self, image):
        """éšæœºè£å‰ªå¹¶è°ƒæ•´å¤§å°"""
        w, h = image.size
        # éšæœºç¼©æ”¾
        scale = random.uniform(0.8, 1.2)
        new_w, new_h = int(w * scale), int(h * scale)
        image = image.resize((new_w, new_h), PIL.Image.LANCZOS)

        # éšæœºè£å‰ª
        if new_w > self.image_size and new_h > self.image_size:
            x = random.randint(0, new_w - self.image_size)
            y = random.randint(0, new_h - self.image_size)
            image = image.crop((x, y, x + self.image_size, y + self.image_size))
        else:
            image = image.resize((self.image_size, self.image_size), PIL.Image.LANCZOS)

        return image

    def create_training_pair(self, image):
        """åˆ›å»ºè®­ç»ƒå¯¹ - å¢å¼ºç‰ˆæœ¬"""
        # ç›®æ ‡å¤–è§‚ - åº”ç”¨éšæœºå…‰ç…§è°ƒæ•´
        appearance = self.apply_lighting_adjustment(image.clone())

        # é€€åŒ–ç‰ˆæœ¬ - æ›´å¼ºçš„å˜æ¢
        degradation = self.create_degradation_version(image.clone())

        # èƒŒæ™¯ - å¤šæ ·åŒ–çš„èƒŒæ™¯ç”Ÿæˆ
        background = self.create_diverse_background(image.clone())

        # æ©ç  - æ›´çœŸå®çš„æ©ç ç”Ÿæˆ
        mask = self.create_advanced_mask(image.clone())

        return appearance, degradation, background, mask

    def apply_lighting_adjustment(self, image):
        """åº”ç”¨å…‰ç…§è°ƒæ•´"""
        # éšæœºé€‰æ‹©ä¸€ç§å…‰ç…§è°ƒæ•´æ–¹å¼
        method = random.choice(['brightness', 'contrast', 'color_temp', 'mixed'])

        if method == 'brightness':
            # è°ƒæ•´äº®åº¦
            brightness_factor = random.uniform(0.7, 1.3)
            image = image * brightness_factor
        elif method == 'contrast':
            # è°ƒæ•´å¯¹æ¯”åº¦
            mean = image.mean()
            contrast_factor = random.uniform(0.8, 1.2)
            image = (image - mean) * contrast_factor + mean
        elif method == 'color_temp':
            # è°ƒæ•´è‰²æ¸©
            if random.random() > 0.5:
                # æš–è‰²è°ƒ
                warm_filter = torch.tensor([1.2, 1.0, 0.8]).view(3, 1, 1)
            else:
                # å†·è‰²è°ƒ
                cool_filter = torch.tensor([0.8, 0.9, 1.2]).view(3, 1, 1)
            image = image * warm_filter if 'warm' in locals() else image * cool_filter
        else:  # mixed
            # æ··åˆè°ƒæ•´
            brightness_factor = random.uniform(0.8, 1.2)
            contrast_factor = random.uniform(0.9, 1.1)
            mean = image.mean()
            image = image * brightness_factor
            image = (image - mean) * contrast_factor + mean

        return torch.clamp(image, -1, 1)

    def create_degradation_version(self, image):
        """åˆ›å»ºé€€åŒ–ç‰ˆæœ¬"""
        degradation = image.clone()

        # æ›´å¼ºçš„äº®åº¦è°ƒæ•´
        brightness = random.uniform(0.3, 0.8)
        degradation = degradation * brightness

        # æ·»åŠ å™ªå£°
        if random.random() > 0.3:
            noise_std = random.uniform(0.05, 0.15)
            degradation = degradation + torch.randn_like(degradation) * noise_std

        # æ¨¡ç³Š
        if random.random() > 0.5:
            from torchvision.transforms.functional import gaussian_blur
            kernel_size = random.choice([11, 15, 21])
            degradation = gaussian_blur(degradation.unsqueeze(0), kernel_size=kernel_size)[0]

        return torch.clamp(degradation, -1, 1)

    def create_diverse_background(self, image):
        """åˆ›å»ºå¤šæ ·åŒ–èƒŒæ™¯"""
        method = random.choice(['blur', 'color', 'texture', 'composite'])

        if method == 'blur':
            from torchvision.transforms.functional import gaussian_blur
            background = gaussian_blur(image.unsqueeze(0), kernel_size=51)[0]
        elif method == 'color':
            # çº¯è‰²èƒŒæ™¯
            bg_color = torch.rand(3, 1, 1) * 2 - 1
            background = bg_color.repeat(1, image.shape[1], image.shape[2])
        elif method == 'texture':
            # çº¹ç†èƒŒæ™¯
            background = torch.randn_like(image) * 0.3
        else:  # composite
            # æ··åˆèƒŒæ™¯
            from torchvision.transforms.functional import gaussian_blur
            blurred = gaussian_blur(image.unsqueeze(0), kernel_size=51)[0]
            noise = torch.randn_like(image) * 0.2
            background = blurred * 0.7 + noise * 0.3

        return torch.clamp(background, -1, 1)

    def create_advanced_mask(self, image):
        """åˆ›å»ºé«˜çº§æ©ç """
        h, w = image.shape[1], image.shape[2]
        mask = torch.zeros(1, h, w)  # ç›´æ¥åˆ›å»º3ç»´å¼ é‡

        # éšæœºé€‰æ‹©æ©ç ç±»å‹
        mask_type = random.choice(['ellipse', 'rectangle', 'irregular', 'gradient'])

        if mask_type == 'ellipse':
            # æ¤­åœ†æ©ç 
            center_y, center_x = h // 2, w // 2
            ellipse_h = random.randint(h // 4, 3 * h // 4)
            ellipse_w = random.randint(w // 4, 3 * w // 4)
            y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
            mask_data = ((x - center_x) ** 2 / (ellipse_w // 2) ** 2 +
                         (y - center_y) ** 2 / (ellipse_h // 2) ** 2) <= 1
            mask[0] = mask_data.float()

        elif mask_type == 'rectangle':
            # çŸ©å½¢æ©ç 
            rect_h = random.randint(h // 3, 2 * h // 3)
            rect_w = random.randint(w // 3, 2 * w // 3)
            start_y = random.randint(0, h - rect_h)
            start_x = random.randint(0, w - rect_w)
            mask[0, start_y:start_y + rect_h, start_x:start_x + rect_w] = 1

        elif mask_type == 'irregular':
            # ä¸è§„åˆ™æ©ç 
            center_y, center_x = h // 2, w // 2
            y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
            distance = torch.sqrt((x - center_x).float() ** 2 + (y - center_y).float() ** 2)
            max_dist = torch.sqrt(torch.tensor(center_x ** 2 + center_y ** 2))
            mask_data = (distance < max_dist * random.uniform(0.3, 0.6)).float()
            mask[0] = mask_data

        else:  # gradient
            # æ¸å˜æ©ç 
            center_y, center_x = h // 2, w // 2
            y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
            distance = torch.sqrt((x - center_x).float() ** 2 + (y - center_y).float() ** 2)
            max_dist = torch.sqrt(torch.tensor(center_x ** 2 + center_y ** 2))
            mask_data = torch.exp(-distance / (max_dist * random.uniform(0.3, 0.7)))
            mask[0] = mask_data

        # æ·»åŠ å™ªå£°ä½¿è¾¹ç¼˜æ›´è‡ªç„¶
        if random.random() > 0.3:
            noise = torch.randn(h, w) * 0.1
            mask[0] = torch.clamp(mask[0] + noise, 0, 1)

        return mask  # å½¢çŠ¶: [1, H, W]

    def create_environment_map(self, image):
        """åŸºäºå›¾åƒå†…å®¹åˆ›å»ºç¯å¢ƒå›¾"""
        # è®¡ç®—å›¾åƒçš„ä¸»è¦é¢œè‰²
        avg_color = image.mean(dim=(1, 2)).cpu().numpy()

        # åˆ›å»ºåŸºç¡€ç¯å¢ƒå›¾
        env_map = np.ones((32, 32, 3)) * avg_color.reshape(1, 1, 3)

        # æ·»åŠ å…‰ç…§å˜åŒ–
        light_intensity = random.uniform(0.8, 1.2)
        light_color = np.random.uniform(0.8, 1.2, 3)
        env_map = env_map * light_intensity * light_color.reshape(1, 1, 3)

        # æ·»åŠ éšæœºå˜åŒ–
        env_map += np.random.normal(0, 0.1, (32, 32, 3))

        # ç¡®ä¿å½¢çŠ¶ä¸º [32, 32, 3]
        env_map = np.clip(env_map, 0, 1)
        env_map_tensor = torch.from_numpy(env_map).float()

        return env_map_tensor  # å½¢çŠ¶: [32, 32, 3]

    def create_default_sample(self):
        """åˆ›å»ºé»˜è®¤æ ·æœ¬"""
        # ç¡®ä¿æ‰€æœ‰å¼ é‡æœ‰æ­£ç¡®çš„å½¢çŠ¶
        image_tensor = torch.rand(3, self.image_size, self.image_size) * 2 - 1
        appearance = image_tensor
        degradation = image_tensor * 0.5
        background = torch.randn_like(image_tensor) * 0.3
        mask = torch.ones(1, self.image_size, self.image_size) * 0.5  # [1, H, W]
        environment_map = torch.rand(32, 32, 3)  # [32, 32, 3]
        prompt = "professional studio lighting"

        return {
            'appearance': appearance,  # [3, 512, 512]
            'degradation': degradation,  # [3, 512, 512]
            'background': background,  # [3, 512, 512]
            'mask': mask,  # [1, 512, 512]
            'environment_map': environment_map,  # [32, 32, 3]
            'prompt': prompt
        }# 3. å¢å¼ºçš„æŸå¤±å‡½æ•°ç±»
class ICLightLoss(nn.Module):
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01):
        super().__init__()
        self.alpha = alpha  # æ‰©æ•£æŸå¤±æƒé‡
        self.beta = beta  # å…‰ä¼ è¾“ä¸€è‡´æ€§æŸå¤±æƒé‡
        self.gamma = gamma  # æ„ŸçŸ¥æŸå¤±æƒé‡

    def forward(self, noise_pred, noise_target, appearances=None, generated=None):
        """è®¡ç®—æ€»æŸå¤±"""
        # åŸºç¡€æ‰©æ•£æŸå¤±
        diffusion_loss = F.mse_loss(noise_pred, noise_target)

        total_loss = self.alpha * diffusion_loss

        # å…‰ä¼ è¾“ä¸€è‡´æ€§æŸå¤±ï¼ˆå¦‚æœæœ‰å¤šä¸ªå…‰ç…§æ¡ä»¶ï¼‰
        if appearances is not None and len(appearances) > 1:
            consistency_loss = self.light_transport_consistency_loss(appearances)
            total_loss += self.beta * consistency_loss

        # æ„ŸçŸ¥æŸå¤±ï¼ˆå¦‚æœç”Ÿæˆäº†å›¾åƒï¼‰
        if generated is not None and appearances is not None:
            perceptual_loss = self.perceptual_similarity_loss(generated, appearances[0])
            total_loss += self.gamma * perceptual_loss

        return total_loss, {
            'diffusion_loss': diffusion_loss.item(),
            'total_loss': total_loss.item()
        }

    def light_transport_consistency_loss(self, appearances):
        """å…‰ä¼ è¾“ä¸€è‡´æ€§æŸå¤±"""
        # è¿™é‡Œå®ç°è®ºæ–‡ä¸­çš„å…‰ä¼ è¾“ä¸€è‡´æ€§çº¦æŸ
        # å¯¹äºåŒä¸€ç‰©ä½“çš„ä¸åŒå…‰ç…§å¤–è§‚ï¼Œåº”è¯¥æ»¡è¶³çº¿æ€§æ··åˆå…³ç³»
        if len(appearances) < 2:
            return 0.0

        # ç®€åŒ–å®ç°ï¼šç¡®ä¿å¤–è§‚å˜åŒ–å¹³æ»‘
        loss = 0.0
        for i in range(len(appearances) - 1):
            loss += F.l1_loss(appearances[i], appearances[i + 1])

        return loss / (len(appearances) - 1)

    def perceptual_similarity_loss(self, generated, target):
        """æ„ŸçŸ¥ç›¸ä¼¼æ€§æŸå¤±"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨é«˜çº§ç‰¹å¾ç›¸ä¼¼æ€§
        return F.l1_loss(generated, target)


# 4. å¢å¼ºçš„è®­ç»ƒå™¨ç±»
class ICLightTrainer:
    def __init__(self, model_name="runwayml/stable-diffusion-v1-5"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # å¯ç”¨å†…å­˜ä¼˜åŒ–
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()

        # åŠ è½½æ¨¡å‹ç»„ä»¶
        print("åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç»„ä»¶...")

        self.vae = AutoencoderKL.from_pretrained(
            model_name, subfolder="vae", torch_dtype=torch.float16
        ).to(self.device)

        self.unet = UNet2DConditionModel.from_pretrained(
            model_name, subfolder="unet", torch_dtype=torch.float16
        ).to(self.device)

        self.text_encoder = CLIPTextModel.from_pretrained(
            model_name, subfolder="text_encoder", torch_dtype=torch.float16
        ).to(self.device)

        self.tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")

        # å†»ç»“VAEå’Œæ–‡æœ¬ç¼–ç å™¨
        self.vae.requires_grad_(False)
        self.text_encoder.requires_grad_(False)

        # ç¯å¢ƒç¼–ç å™¨ - ç°åœ¨è¿™ä¸ªç±»å·²ç»å®šä¹‰åœ¨å‰é¢äº†
        self.env_encoder = EnvironmentEncoder().to(self.device)

        # ä¿®æ”¹UNetè¾“å…¥å±‚
        self.modify_unet_input()

        # æ¢¯åº¦æ£€æŸ¥ç‚¹
        self.unet.enable_gradient_checkpointing()

        # æŸå¤±å‡½æ•°
        self.criterion = ICLightLoss(alpha=1.0, beta=0.1, gamma=0.01)

        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            list(self.unet.parameters()) + list(self.env_encoder.parameters()),
            lr=1e-5, weight_decay=1e-4, betas=(0.9, 0.999)
        )

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100, eta_min=1e-6
        )

        # å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = DDPMScheduler.from_pretrained(model_name, subfolder="scheduler")

        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        self.print_memory_usage()

    def print_memory_usage(self):
        """æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024 ** 3
            reserved = torch.cuda.memory_reserved() / 1024 ** 3
            print(f"GPUå†…å­˜ä½¿ç”¨: {allocated:.2f} GB / {reserved:.2f} GB")

    def modify_unet_input(self):
        """ä¿®æ”¹UNetè¾“å…¥å±‚"""
        original_conv = self.unet.conv_in
        new_conv = nn.Conv2d(13, original_conv.out_channels, kernel_size=3, padding=1).to(self.device)

        with torch.no_grad():
            new_conv.weight[:, :4] = original_conv.weight
            new_conv.bias = original_conv.bias

        self.unet.conv_in = new_conv
        print(f"ä¿®æ”¹UNetè¾“å…¥é€šé“: 4 -> 13")

    def encode_images(self, images):
        """ç¼–ç å›¾åƒåˆ°æ½œåœ¨ç©ºé—´"""
        if images.dim() == 3:
            images = images.unsqueeze(0)

        with torch.no_grad():
            with torch.autocast(device_type='cuda', dtype=torch.float16):
                latents = self.vae.encode(images.half()).latent_dist.sample()
                latents = latents * self.vae.config.scaling_factor
        return latents.float()

    def prepare_extra_conditions(self, degradation, background, mask):
        """å‡†å¤‡é¢å¤–æ¡ä»¶"""
        with torch.no_grad():
            # ç¼–ç é€€åŒ–å›¾åƒå’ŒèƒŒæ™¯å›¾åƒ
            degradation_latent = self.encode_images(degradation)  # [batch, 4, 64, 64]
            background_latent = self.encode_images(background)  # [batch, 4, 64, 64]

            # ç¡®ä¿maskæ˜¯4ç»´çš„ [batch, 1, H, W]
            if mask.dim() == 3:
                mask = mask.unsqueeze(1)  # ä» [batch, H, W] å˜æˆ [batch, 1, H, W]

            # è°ƒæ•´maskå¤§å°åˆ°64Ã—64
            mask_resized = F.interpolate(
                mask,  # ç°åœ¨å½¢çŠ¶æ˜¯ [batch, 1, H, W]
                size=degradation_latent.shape[-2:],  # (64, 64)
                mode='bilinear',  # åŒçº¿æ€§æ’å€¼
                align_corners=False
            )

            # æ‹¼æ¥æ‰€æœ‰æ¡ä»¶
            extra_conditions = torch.cat([
                degradation_latent,  # [batch, 4, 64, 64]
                background_latent,  # [batch, 4, 64, 64]
                mask_resized  # [batch, 1, 64, 64]
            ], dim=1)  # æ€»å…± 4+4+1 = 9ä¸ªé€šé“

            print(f"é¢å¤–æ¡ä»¶å½¢çŠ¶: {extra_conditions.shape}")  # è°ƒè¯•ç”¨
            return extra_conditions  # [batch, 9, 64, 64]

    def train_step(self, batch):
        """è®­ç»ƒæ­¥éª¤"""
        self.unet.train()
        self.env_encoder.train()

        torch.cuda.empty_cache()

        try:
            # å‡†å¤‡æ•°æ®
            appearance = batch['appearance'].to(self.device)
            degradation = batch['degradation'].to(self.device)
            background = batch['background'].to(self.device)
            mask = batch['mask'].to(self.device)
            environment_map = batch['environment_map'].to(self.device)
            prompts = batch['prompt']

            batch_size = appearance.shape[0]

            # ç¡®ä¿maskæ˜¯4ç»´çš„ [batch, 1, H, W]
            if mask.dim() == 3:
                mask = mask.unsqueeze(1)  # æ·»åŠ é€šé“ç»´åº¦
            elif mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡å’Œé€šé“ç»´åº¦

            # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰è¾“å…¥å½¢çŠ¶
            print(f"\n=== è®­ç»ƒæ­¥éª¤è¾“å…¥å½¢çŠ¶ ===")
            print(f"appearance: {appearance.shape}")
            print(f"degradation: {degradation.shape}")
            print(f"background: {background.shape}")
            print(f"mask: {mask.shape}")
            print(f"environment_map: {environment_map.shape}")

            # ç¼–ç ç›®æ ‡å›¾åƒ
            with torch.no_grad():
                target_latents = self.encode_images(appearance)
                print(f"target_latents: {target_latents.shape}")

            # å‡†å¤‡é¢å¤–æ¡ä»¶
            extra_conditions = self.prepare_extra_conditions(degradation, background, mask)

            # ç¼–ç ç¯å¢ƒå…‰ç…§
            env_embeddings = self.env_encoder(environment_map)
            print(f"env_embeddings: {env_embeddings.shape}")

            # ç¼–ç æ–‡æœ¬æç¤º
            with torch.no_grad():
                text_inputs = self.tokenizer(
                    prompts, padding="max_length", max_length=77,
                    truncation=True, return_tensors="pt"
                )
                text_embeddings = self.text_encoder(text_inputs.input_ids.to(self.device))[0]
                print(f"text_embeddings: {text_embeddings.shape}")

            # ç»“åˆåµŒå…¥
            combined_embeddings = text_embeddings.clone()
            combined_embeddings[:, :3] = env_embeddings
            print(f"combined_embeddings: {combined_embeddings.shape}")

            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(target_latents)
            timesteps = torch.randint(
                0, self.noise_scheduler.config.num_train_timesteps,
                (batch_size,), device=self.device
            ).long()

            noisy_latents = self.noise_scheduler.add_noise(target_latents, noise, timesteps)
            print(f"noisy_latents: {noisy_latents.shape}")

            # å‡†å¤‡UNetè¾“å…¥
            unet_input = torch.cat([noisy_latents, extra_conditions], dim=1)
            print(f"unet_input: {unet_input.shape}")  # åº”è¯¥æ˜¯ [batch, 13, 64, 64]

            # é¢„æµ‹å™ªå£°
            with torch.autocast(device_type='cuda', dtype=torch.float16):
                noise_pred = self.unet(
                    unet_input, timesteps, encoder_hidden_states=combined_embeddings
                ).sample

                print(f"noise_pred: {noise_pred.shape}")

                # è®¡ç®—æŸå¤±
                total_loss, loss_dict = self.criterion(
                    noise_pred, noise,
                    appearances=[appearance]
                )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.unet.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(self.env_encoder.parameters(), 1.0)

            self.optimizer.step()

            return total_loss.item(), loss_dict

        except RuntimeError as e:
            if "out of memory" in str(e):
                print("GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                torch.cuda.empty_cache()
                return 0.0, {}
            else:
                print(f"è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                return 0.0, {}

    def train(self, dataloader, num_epochs=10, save_interval=2):
        """è®­ç»ƒå¾ªç¯"""
        print("å¼€å§‹è®­ç»ƒ...")

        best_loss = float('inf')

        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            loss_history = {'diffusion_loss': 0.0, 'total_loss': 0.0}

            # è¿›åº¦æ¡
            from tqdm import tqdm
            pbar = tqdm(dataloader, desc=f'Epoch {epoch}/{num_epochs}')

            for batch_idx, batch in enumerate(pbar):
                loss, loss_dict = self.train_step(batch)

                if loss > 0:
                    epoch_loss += loss
                    num_batches += 1

                    # æ›´æ–°æŸå¤±å†å²
                    for k, v in loss_dict.items():
                        if k in loss_history:
                            loss_history[k] += v

                    # æ›´æ–°è¿›åº¦æ¡
                    if num_batches > 0:
                        avg_loss = epoch_loss / num_batches
                        pbar.set_postfix({
                            'Loss': f'{avg_loss:.4f}',
                            'Diff': f'{loss_history["diffusion_loss"] / num_batches:.4f}'
                        })

            if num_batches > 0:
                avg_loss = epoch_loss / num_batches
                print(f'Epoch {epoch} å®Œæˆ. å¹³å‡æŸå¤±: {avg_loss:.4f}')

                # æ›´æ–°å­¦ä¹ ç‡
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f'å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}')

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    self.save_checkpoint(epoch, avg_loss, "best")
                    print(f'æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒæŸå¤±: {best_loss:.4f}')

                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % save_interval == 0:
                    self.save_checkpoint(epoch, avg_loss)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(num_epochs, avg_loss, "final")
        print("è®­ç»ƒå®Œæˆï¼")

    def save_checkpoint(self, epoch, loss, prefix="checkpoint"):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'loss': loss,
            'unet_state_dict': self.unet.state_dict(),
            'env_encoder_state_dict': self.env_encoder.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
        }

        filename = f"{prefix}_epoch_{epoch}.pth"
        torch.save(checkpoint, filename)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")


# 5. ä¸»è®­ç»ƒå‡½æ•°
def main():
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

    # æ£€æŸ¥è®­ç»ƒå›¾åƒ
    data_dir = "./training_images"
    if not os.path.exists(data_dir):
        print(f"åˆ›å»ºè®­ç»ƒå›¾åƒç›®å½•: {data_dir}")
        os.makedirs(data_dir)
        print(f"è¯·å°†è®­ç»ƒå›¾åƒæ”¾å…¥ {data_dir} ç›®å½•ï¼Œç„¶åé‡æ–°è¿è¡Œ")
        return

    image_files = glob(os.path.join(data_dir, "*.*"))
    if not image_files:
        print(f"åœ¨ {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶ï¼")
        return

    print(f"æ‰¾åˆ° {len(image_files)} ä¸ªè®­ç»ƒå›¾åƒ")

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = ICLightDataset(data_dir, max_samples=min(300, len(image_files)))
    dataloader = DataLoader(
        dataset,
        batch_size=10,  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )

    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = ICLightTrainer()

    # å¼€å§‹è®­ç»ƒ
    trainer.train(dataloader, num_epochs=10, save_interval=2)

if __name__ == "__main__":
    main()